var documenterSearchIndex = {"docs":
[{"location":"api/","page":"Reference","title":"Reference","text":"CurrentModule = MPIMapReduce","category":"page"},{"location":"api/#MPIMapReduce.jl","page":"Reference","title":"MPIMapReduce.jl","text":"","category":"section"},{"location":"api/","page":"Reference","title":"Reference","text":"Modules = [MPIMapReduce]","category":"page"},{"location":"api/#MPIMapReduce.Cat","page":"Reference","title":"MPIMapReduce.Cat","text":"Cat(dims)\n\nOperator that performs a concatenation over the dimensions dims.\n\nExamples\n\njulia> c1 = MPIMapReduce.Cat(1);\n\njulia> c1([1], [2])\n2-element Vector{Int64}:\n 1\n 2\n\njulia> c2 = MPIMapReduce.Cat(2);\n\njulia> c2([1], [2])\n1×2 Matrix{Int64}:\n 1  2\n\n\n\n\n\n","category":"type"},{"location":"api/#MPIMapReduce.pmapgatherv-Tuple{Any, Any, Vararg{Any, N} where N}","page":"Reference","title":"MPIMapReduce.pmapgatherv","text":"pmapgatherv(f, op, iterators...; [root = 0], [comm = MPI.COMM_WORLD])\n\nApply function f to each element(s) in iterators, and then reduce the result using the concatenation operator op. Both the map and the reduction are evaluated in parallel over the processes corresponding to the communicator comm. The result of the operation is returned at root, while nothing is returned at the other processes.\n\nThe result of pmapgatherv is equivalent to that of mapreduce(f, op, iterators...).\n\nThe concatenation operator op may be one of vcat, hcat and Cat.\n\nwarn: Warn\nAnonymous functions are not supported as the concatenation operator.\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIMapReduce.pmapreduce-Tuple{Any, Any, Vararg{Any, N} where N}","page":"Reference","title":"MPIMapReduce.pmapreduce","text":"pmapreduce(f, op, iterators...; [root = 0], [comm = MPI.COMM_WORLD])\n\nApply function f to each element(s) in iterators, and then reduce the result using the elementwise binary reduction operator op. Both the map and the reduction are evaluated in parallel over the processes corresponding to the communicator comm. The result of the operation is returned at root, while nothing is returned at the other processes.\n\nThe result of pmapreduce is equivalent to that of mapreduce(f, (x, y) -> op.(x, y), iterators...).\n\nnote: Note\nUnlike the standard mapreduce operation in Julia, this operation is performed elementwise on the arrays returned from the various processes. The returned value must be compatible with the elementwise operation.\n\nnote: Note\nMPI reduction operators require each worker to return only one array with an eltype T that satisfies isbitstype(T) == true. Such a limitation does not exist for Julia operators.\n\n\n\n\n\n","category":"method"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Reduction","page":"Examples","title":"Reduction","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"We may compute the elementwise sum of arrays on each processor as","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using MPIMapReduce\nusing MPI\nMPI.Init()\n\ny1 = pmapreduce(x -> ones(2) * x, +, 1:5)\n\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    show(stdout, MIME\"text/plain\"(), y1)\n    println()\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This produces the output","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"2-element Array{Float64,1}:\n 15.0\n 15.0","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We may compute the elementwise product of arrays on each processor as","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using MPIMapReduce\nusing MPI\nMPI.Init()\n\ny1 = pmapreduce(x -> ones(2, 2) * x, *, 1:4)\n\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    show(stdout, MIME\"text/plain\"(), y1)\n    println()\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This produces the output","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"2×2 Array{Float64,2}:\n 24.0  24.0\n 24.0  24.0","category":"page"},{"location":"examples/#Concatenation","page":"Examples","title":"Concatenation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The function pmapgatherv performs concatenates the values returned by the map. The operators vcat and hcat may be provided as the reduction operator to carry out concatenations along the first or the second axis respectively.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For example, we may concatenate Vectors along the second dimension as","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using MPIMapReduce\nusing MPI\nMPI.Init()\n\ny = pmapgatherv(x -> ones(2) * x^2, hcat, 1:5)\n\nif MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    show(stdout, MIME\"text/plain\"(), y)\n    println()\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This leads to the output","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"2×5 Array{Float64,2}:\n 1.0  4.0  9.0  16.0  25.0\n 1.0  4.0  9.0  16.0  25.0","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For more general concatenations, use the reduction operator Cat(dims), where dims refers to the dimensions along which the concatenation is to be carried out. For example, we may concatenate numbers along the 1st and 2nd dimensions to generate a diagonal matrix as","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using MPIMapReduce\nusing MPI\nMPI.Init()\n\ny = pmapgatherv(x -> x^2, Cat([1,2]), 1:3)\n\ncomm = MPI.COMM_WORLD\n\nif MPI.Comm_rank(comm) == 0\n    show(stdout, MIME\"text/plain\"(), y)\n    println()\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"which leads to the output","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"3×3 Array{Int64,2}:\n 1  0  0\n 0  4  0\n 0  0  9","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"note: Note\nAnonymous functions can not be provided as the concatenation operator.","category":"page"},{"location":"#MPIMapReduce","page":"MPIMapReduce","title":"MPIMapReduce","text":"","category":"section"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"This package provides a function pmapreduce that performs a distributed mapreduce operation using MPI.","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"note: Note\nThe map operation is performed in batches, therefore the operation is not load-balanced. The iterators are split evenly over the available processes, however this might change in the future.","category":"page"},{"location":"#Installation","page":"MPIMapReduce","title":"Installation","text":"","category":"section"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"Install the package using","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"julia> using Pkg\n\njulia> Pkg.add(\"https://github.com/jishnub/MPIMapReduce.jl\")","category":"page"},{"location":"#Usage","page":"MPIMapReduce","title":"Usage","text":"","category":"section"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"To start using the package, load it and initialize MPI.","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"using MPIMapReduce\n\nMPI.Init()","category":"page"},{"location":"#Mapreduce","page":"MPIMapReduce","title":"Mapreduce","text":"","category":"section"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"The syntax for a parallel mapreduce is similar to that of a serial mapreduce, but not exactly the same. Given a mapping function f and a binary elementwise reduction operator op, a parallel mapreduce call would look like","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"pmapreduce(f, op, iterators...; [root = 0], [comm = MPI.COMM_WORLD])","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"Optionally the root process and the communicator may be specified as the keyword arguments root and comm. The result of the mapreduce operation is returned at the root process, while nothing is returned at the other processes.","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"note: Note\nUnlike mapreduce, the operator op acts elementwise on the returned values. The operation pmapreduce(f, op, iterators...) is therefore equivalent to mapreduce(f, (x,y) -> op.(x,y), iterators...).","category":"page"},{"location":"#Concatenation","page":"MPIMapReduce","title":"Concatenation","text":"","category":"section"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"The function pmapgatherv may be used to perform a concatenation. The syntax of pmapgatherv is similar to pmapreduce, except the reduction operator is not applied elementwise. Supported reduction operators are vcat, hcat and Cat, where the last operator may be used to perform general concatenations along arbitrary dimensions.","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"To perform a concatenation, use","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"pmapgatherv(f, op, iterators...; [root = 0], [comm = MPI.COMM_WORLD])","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"As in pmapreduce, the concatenated result is returned at root and nothing is returned at the other processes.","category":"page"},{"location":"#Running-scripts","page":"MPIMapReduce","title":"Running scripts","text":"","category":"section"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"Like most MPI code, scripts using MPIMapReduce must be run as","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"$ mpiexec -np <np> <julia> <script.jl>","category":"page"},{"location":"","page":"MPIMapReduce","title":"MPIMapReduce","text":"where <np> is the number of processes, <julia> refers to the path to the julia executable, and <script.jl> is the julia script that needs to be executed.","category":"page"}]
}
